{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport sys\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndf = pd.read_csv(\"/kaggle/input/animedata/outputfile.csv\")\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:43:40.999735Z","iopub.execute_input":"2021-12-02T07:43:41.000056Z","iopub.status.idle":"2021-12-02T07:43:44.105540Z","shell.execute_reply.started":"2021-12-02T07:43:41.000022Z","shell.execute_reply":"2021-12-02T07:43:44.104606Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:43:44.107637Z","iopub.execute_input":"2021-12-02T07:43:44.107966Z","iopub.status.idle":"2021-12-02T07:43:44.113220Z","shell.execute_reply.started":"2021-12-02T07:43:44.107924Z","shell.execute_reply":"2021-12-02T07:43:44.112276Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Dataset():\n    \n    def __init__(self,df):\n        self.df = df\n        self.characters, self.unique_characters = self.load_characters()\n        self.index_to_char = dict((int(i), c) for i, c in enumerate(self.unique_characters))\n        self.char_to_index = dict((c, int(i)) for i, c in enumerate(self.unique_characters))\n        \n        self.maxLen = 150\n        \n        self.x ,self.y = self.x_y_data()\n        \n        self.dataloader = self.batch_data(self.x, self.y, self.maxLen, 256)\n        \n    \n    def load_characters(self):\n        # lowercase all\n        df = self.df.drop_duplicates(subset=['synopsis'])\n        df['synopsis'] = df.synopsis.replace('\\n','', regex=True)\n        df['synopsis'] = df.synopsis.replace('\\r','', regex=True)\n        df['synopsis'] = df.synopsis.replace('[^\\w\\s]','', regex=True)\n\n        text = df['synopsis'][df['synopsis'].map(len)>150]\n        \n        text = text.sample(frac = 0.5)\n\n        chars = sorted(list(set(''.join(text))))\n\n        for c in range(len(chars)):\n            if chars[c] == \"z\":\n                for i in chars[c+1:]:\n                    text = text.str.replace(i,'')\n        \n        chars = sorted(list(set(''.join(text))))\n        \n        print(len(chars))\n        return text, chars\n\n    def id_sentence(self, sentence):\n        return [self.char_to_index[char] for char in sentence]\n            \n    \n    def x_y_data(self):\n        # cut the text in semi-redundant sequences of maxlen characters\n        maxlen = 150\n        step = 1\n        sentences = []\n        next_chars = []\n        for x in self.characters:\n            for i in range(0, len(x) - maxlen, step):\n                sentences.append(self.id_sentence(x[i: i + maxlen]))\n                next_chars.append(self.id_sentence(x[i + maxlen]))\n        print('nb sequences:', len(sentences))\n        \n        return sentences, next_chars\n    \n    def batch_data(self, x,y, sequence_length, batch_size):\n        \"\"\"\n        Batch the neural network data using DataLoader\n        :param words: The word ids of the scripts\n        :param sequence_length: The sequence length of each batch\n        :param batch_size: The size of each batch; the number of sequences in a batch\n        :return: DataLoader with batched data\n        \"\"\"\n        # TODO: Implement function\n\n        data = TensorDataset(torch.Tensor(np.asarray(x)), torch.Tensor(np.asarray(y)))\n        \n        data_loader = torch.utils.data.DataLoader(data, shuffle=False, batch_size=batch_size)\n\n        # return a dataloader\n        return data_loader\n    \n    def __len__(self):\n        return self.x.shape[0]\n    \n    def __getitem__(self,idx):\n        return self.x[idx],self.y[idx]","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:43:44.114618Z","iopub.execute_input":"2021-12-02T07:43:44.114855Z","iopub.status.idle":"2021-12-02T07:43:44.136514Z","shell.execute_reply.started":"2021-12-02T07:43:44.114828Z","shell.execute_reply":"2021-12-02T07:43:44.135547Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"[1,2,3,4,5,6]\n\n[\n    [1,2,3],\n    [2,3,4],\n    [3,4,5],\n    [4,5,6]\n]\n\n[\n    [4],\n    [5],\n    [6],\n    [END]\n]","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:05:04.942935Z","iopub.execute_input":"2021-12-02T07:05:04.943242Z","iopub.status.idle":"2021-12-02T07:05:04.955561Z","shell.execute_reply.started":"2021-12-02T07:05:04.943217Z","shell.execute_reply":"2021-12-02T07:05:04.954982Z"}}},{"cell_type":"code","source":"import pickle\n# dataset = Dataset(df)\n\n# #ensure data persistence, because we arent using the full dataset\n# with open('dataset_65.pkl', 'wb') as outp:\n#     pickle.dump(dataset, outp, pickle.HIGHEST_PROTOCOL)\n\nwith open('../input/chardatasetclass65/dataset_65.pkl', 'rb') as inp:\n    dataset = pickle.load(inp)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:57:09.652236Z","iopub.execute_input":"2021-12-02T07:57:09.653886Z","iopub.status.idle":"2021-12-02T07:57:46.181437Z","shell.execute_reply.started":"2021-12-02T07:57:09.653825Z","shell.execute_reply":"2021-12-02T07:57:46.180413Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"dataset.dataloader","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:57:46.184736Z","iopub.execute_input":"2021-12-02T07:57:46.185277Z","iopub.status.idle":"2021-12-02T07:57:46.192048Z","shell.execute_reply.started":"2021-12-02T07:57:46.185229Z","shell.execute_reply":"2021-12-02T07:57:46.191177Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass LSTMCustom(nn.Module):\n    \n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n        \"\"\"\n        Initialize the PyTorch RNN Module\n        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n        :param output_size: The number of output dimensions of the neural network\n        :param embedding_dim: The size of embeddings, should you choose to use them        \n        :param hidden_dim: The size of the hidden layer outputs\n        :param dropout: dropout to add in between LSTM/GRU layers\n        \"\"\"\n        super(LSTMCustom, self).__init__()\n        \n        # TODO: Implement function\n        \n        # set class variables\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # define model layers\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n        \n        # drpout layer\n        self.dropout = nn.Dropout(dropout)        \n\n        # linear layer\n        self.fc = nn.Linear(hidden_dim, output_size)\n    \n    def forward(self, nn_input, hidden):\n        \"\"\"\n        Forward propagation of the neural network\n        :param nn_input: The input to the neural network\n        :param hidden: The hidden state        \n        :return: Two Tensors, the output of the neural network and the latest hidden state\n        \"\"\"\n        # TODO: Implement function  \n        batch_size = nn_input.size(0)\n\n        # embeddings and lstm_out\n        embeds = self.embedding(nn_input)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        output = self.dropout(lstm_out)\n        output = self.fc(output)\n        \n        # reshape to be batch_size first\n        output = output.view(batch_size, -1, self.output_size)\n        out = output[:, -1, :] # get last batch of labels       \n        # return one batch of output word scores and the hidden state\n        return nn.functional.softmax(out), hidden\n    \n    \n    def init_hidden(self, batch_size):\n        '''\n        Initialize the hidden state of an LSTM/GRU\n        :param batch_size: The batch_size of the hidden state\n        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n        '''\n        # Implement function\n        weight = next(self.parameters()).data\n        \n        # initialize hidden state with zero weights, and move to GPU if available\n            \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n \n        \n        return hidden","metadata":{"execution":{"iopub.status.busy":"2021-12-02T08:02:02.243545Z","iopub.execute_input":"2021-12-02T08:02:02.243881Z","iopub.status.idle":"2021-12-02T08:02:02.261271Z","shell.execute_reply.started":"2021-12-02T08:02:02.243848Z","shell.execute_reply":"2021-12-02T08:02:02.260457Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def save_model(filename, decoder):\n    torch.save(decoder, filename) ","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:06:56.653177Z","iopub.execute_input":"2021-12-02T07:06:56.653562Z","iopub.status.idle":"2021-12-02T07:06:56.666304Z","shell.execute_reply.started":"2021-12-02T07:06:56.653499Z","shell.execute_reply":"2021-12-02T07:06:56.665696Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches, dataset):\n    dataloader = dataset.dataloader\n    batch_losses = []\n    \n    rnn.train()\n\n    print(\"Training for %d epoch(s)...\" % n_epochs)\n    for epoch_i in range(1, n_epochs + 1):\n        \n        # initialize hidden state\n        hidden = rnn.init_hidden(batch_size)\n        \n        for batch_i, (inputs, labels) in enumerate(dataloader, 1):\n            # make sure you iterate over completely full batches, only\n            n_batches = len(dataloader.dataset)//batch_size\n            if(batch_i > n_batches):\n                break\n            \n            # forward, back prop\n            \n            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs.long(), labels.long().squeeze(), hidden)          \n            # record loss\n            batch_losses.append(loss)\n\n            # printing loss stats\n            if batch_i % show_every_n_batches == 0:\n                print('Epoch: {:>4}/{:<4} Batch:{}/{}  Loss: {}\\n'.format(\n                    epoch_i, n_epochs, batch_i,n_batches,np.average(batch_losses)))\n                batch_losses = []\n        filename = \"lstm_char_v2_{epoch}_{loss}\".format(epoch=epoch_i, loss=loss)\n        save_model(filename, rnn)\n\n    # returns a trained rnn\n    return rnn","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:06:56.667445Z","iopub.execute_input":"2021-12-02T07:06:56.667844Z","iopub.status.idle":"2021-12-02T07:06:56.677535Z","shell.execute_reply.started":"2021-12-02T07:06:56.667814Z","shell.execute_reply":"2021-12-02T07:06:56.676714Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n    \"\"\"\n    Forward and backward propagation on the neural network\n    :param decoder: The PyTorch Module that holds the neural network\n    :param decoder_optimizer: The PyTorch optimizer for the neural network\n    :param criterion: The PyTorch loss function\n    :param inp: A batch of input to the neural network\n    :param target: The target output for the batch of input\n    :return: The loss and the latest hidden state Tensor\n    \"\"\"\n    \n    # TODO: Implement Function\n    \n    # move data to GPU, if available\n    \n    if train_on_gpu:\n        inp, target = inp.cuda(), target.cuda()\n    \n    # perform backpropagation and optimization\n    hidden = tuple([each.data for each in hidden])\n    rnn.zero_grad()\n    output, hidden = rnn(inp, hidden)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n\n    # return the loss over a batch and the hidden state produced by our model\n    return loss.item(), hidden","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:06:56.678703Z","iopub.execute_input":"2021-12-02T07:06:56.679074Z","iopub.status.idle":"2021-12-02T07:06:56.692901Z","shell.execute_reply.started":"2021-12-02T07:06:56.679045Z","shell.execute_reply":"2021-12-02T07:06:56.692330Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Check for a GPU\ntrain_on_gpu = torch.cuda.is_available()\n\ndef predict(dataset, model, next_words):\n    model.eval()\n    text = np.random.choice(dataset.characters)[:150] # select random tweet\n\n#     words = text.split(' ')\n    generated =\"\"\n    print('----- Generating with seed: \"' + text + '\"')\n    sys.stdout.write(generated)\n    for i in range(0, next_words):\n        x = torch.tensor([[dataset.char_to_index[w] for w in text[i:]]])\n        \n        hidden = rnn.init_hidden(1)\n\n        # get the output of the rnn\n        output, _ = rnn(x, hidden)\n        last_word_logits = output[-1]\n\n        \n        word_index = np.random.choice(len(last_word_logits), p=last_word_logits.detach().numpy())\n        generated += \"\" + dataset.index_to_char[word_index]\n        text = text + dataset.index_to_char[word_index]\n        \n        sys.stdout.write(dataset.index_to_char[word_index])\n        sys.stdout.flush()\n    print()\n    \n    return generated\n\ndef train_pred(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, batch_size, dataset, num_epochs, show_every_n_batches, train):\n    if train == True:\n\n        rnn = LSTMCustom(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.3)\n        if train_on_gpu:\n            rnn.cuda()\n\n        # defining loss and optimization functions for training\n        optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n        criterion = nn.CrossEntropyLoss()\n\n        # training the model\n        trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches, dataset)\n\n        # saving the trained model\n        save_model('trained_rnn_epoch_20.pt', trained_rnn)\n        print('Model Trained and Saved')\n    else:\n        rnn = torch.load(\"../input/lstmcharv2/lstm_char_v2_9_1.3241478204727173\",map_location=torch.device('cpu'))\n        predict(dataset, rnn, 500)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-02T08:11:19.317449Z","iopub.execute_input":"2021-12-02T08:11:19.317957Z","iopub.status.idle":"2021-12-02T08:11:19.333085Z","shell.execute_reply.started":"2021-12-02T08:11:19.317910Z","shell.execute_reply":"2021-12-02T08:11:19.332336Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Training parameters\n# Number of Epochs\nnum_epochs = 20\n# Learning Rate\nlearning_rate = 0.001\nbatch_size=256\n# Model parameters\nL = len(dataset.unique_characters)\n# Vocab size\nvocab_size = L\n# Output size\noutput_size = L\n# Embedding Dimension\nembedding_dim = 200\n# Hidden Dimension\nhidden_dim = 256\n# Number of RNN Layers\nn_layers = 2\n# Show stats for every n number of batches\nshow_every_n_batches = 1000\n\ntrain_pred(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, batch_size, dataset, num_epochs, show_every_n_batches, False)","metadata":{"execution":{"iopub.status.busy":"2021-12-02T08:11:27.374657Z","iopub.execute_input":"2021-12-02T08:11:27.375471Z","iopub.status.idle":"2021-12-02T08:11:46.919205Z","shell.execute_reply.started":"2021-12-02T08:11:27.375418Z","shell.execute_reply":"2021-12-02T08:11:46.918096Z"},"trusted":true},"execution_count":29,"outputs":[]}]}
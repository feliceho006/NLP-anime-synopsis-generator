{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T07:43:41.000056Z",
     "iopub.status.busy": "2021-12-02T07:43:40.999735Z",
     "iopub.status.idle": "2021-12-02T07:43:44.105540Z",
     "shell.execute_reply": "2021-12-02T07:43:44.104606Z",
     "shell.execute_reply.started": "2021-12-02T07:43:41.000022Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "df = pd.read_csv(\"/kaggle/input/animedata/outputfile.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T07:43:44.107966Z",
     "iopub.status.busy": "2021-12-02T07:43:44.107637Z",
     "iopub.status.idle": "2021-12-02T07:43:44.113220Z",
     "shell.execute_reply": "2021-12-02T07:43:44.112276Z",
     "shell.execute_reply.started": "2021-12-02T07:43:44.107924Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T07:43:44.114855Z",
     "iopub.status.busy": "2021-12-02T07:43:44.114618Z",
     "iopub.status.idle": "2021-12-02T07:43:44.136514Z",
     "shell.execute_reply": "2021-12-02T07:43:44.135547Z",
     "shell.execute_reply.started": "2021-12-02T07:43:44.114828Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.df = df\n",
    "        self.characters, self.unique_characters = self.load_characters()\n",
    "        self.index_to_char = dict((int(i), c) for i, c in enumerate(self.unique_characters))\n",
    "        self.char_to_index = dict((c, int(i)) for i, c in enumerate(self.unique_characters))\n",
    "        \n",
    "        self.maxLen = 150\n",
    "        \n",
    "        self.x ,self.y = self.x_y_data()\n",
    "        \n",
    "        self.dataloader = self.batch_data(self.x, self.y, self.maxLen, 256)\n",
    "        \n",
    "    \n",
    "    def load_characters(self):\n",
    "        # lowercase all\n",
    "        df = self.df.drop_duplicates(subset=['synopsis'])\n",
    "        df['synopsis'] = df.synopsis.replace('\\n','', regex=True)\n",
    "        df['synopsis'] = df.synopsis.replace('\\r','', regex=True)\n",
    "        df['synopsis'] = df.synopsis.replace('[^\\w\\s]','', regex=True)\n",
    "\n",
    "        text = df['synopsis'][df['synopsis'].map(len)>150]\n",
    "        \n",
    "        text = text.sample(frac = 0.5)\n",
    "\n",
    "        chars = sorted(list(set(''.join(text))))\n",
    "\n",
    "        for c in range(len(chars)):\n",
    "            if chars[c] == \"z\":\n",
    "                for i in chars[c+1:]:\n",
    "                    text = text.str.replace(i,'')\n",
    "        \n",
    "        chars = sorted(list(set(''.join(text))))\n",
    "        \n",
    "        print(len(chars))\n",
    "        return text, chars\n",
    "\n",
    "    def id_sentence(self, sentence):\n",
    "        return [self.char_to_index[char] for char in sentence]\n",
    "            \n",
    "    \n",
    "    def x_y_data(self):\n",
    "        # cut the text in semi-redundant sequences of maxlen characters\n",
    "        maxlen = 150\n",
    "        step = 1\n",
    "        sentences = []\n",
    "        next_chars = []\n",
    "        for x in self.characters:\n",
    "            for i in range(0, len(x) - maxlen, step):\n",
    "                sentences.append(self.id_sentence(x[i: i + maxlen]))\n",
    "                next_chars.append(self.id_sentence(x[i + maxlen]))\n",
    "        print('nb sequences:', len(sentences))\n",
    "        \n",
    "        return sentences, next_chars\n",
    "    \n",
    "    def batch_data(self, x,y, sequence_length, batch_size):\n",
    "        \"\"\"\n",
    "        Batch the neural network data using DataLoader\n",
    "        :param words: The word ids of the scripts\n",
    "        :param sequence_length: The sequence length of each batch\n",
    "        :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "        :return: DataLoader with batched data\n",
    "        \"\"\"\n",
    "        # TODO: Implement function\n",
    "\n",
    "        data = TensorDataset(torch.Tensor(np.asarray(x)), torch.Tensor(np.asarray(y)))\n",
    "        \n",
    "        data_loader = torch.utils.data.DataLoader(data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        # return a dataloader\n",
    "        return data_loader\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T07:05:04.943242Z",
     "iopub.status.busy": "2021-12-02T07:05:04.942935Z",
     "iopub.status.idle": "2021-12-02T07:05:04.955561Z",
     "shell.execute_reply": "2021-12-02T07:05:04.954982Z",
     "shell.execute_reply.started": "2021-12-02T07:05:04.943217Z"
    }
   },
   "source": [
    "[1,2,3,4,5,6]\n",
    "\n",
    "[\n",
    "    [1,2,3],\n",
    "    [2,3,4],\n",
    "    [3,4,5],\n",
    "    [4,5,6]\n",
    "]\n",
    "\n",
    "[\n",
    "    [4],\n",
    "    [5],\n",
    "    [6],\n",
    "    [END]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T07:57:09.653886Z",
     "iopub.status.busy": "2021-12-02T07:57:09.652236Z",
     "iopub.status.idle": "2021-12-02T07:57:46.181437Z",
     "shell.execute_reply": "2021-12-02T07:57:46.180413Z",
     "shell.execute_reply.started": "2021-12-02T07:57:09.653825Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# dataset = Dataset(df)\n",
    "\n",
    "# #ensure data persistence, because we arent using the full dataset\n",
    "# with open('dataset_65.pkl', 'wb') as outp:\n",
    "#     pickle.dump(dataset, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../input/chardatasetclass65/dataset_65.pkl', 'rb') as inp:\n",
    "    dataset = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T07:57:46.185277Z",
     "iopub.status.busy": "2021-12-02T07:57:46.184736Z",
     "iopub.status.idle": "2021-12-02T07:57:46.192048Z",
     "shell.execute_reply": "2021-12-02T07:57:46.191177Z",
     "shell.execute_reply.started": "2021-12-02T07:57:46.185229Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T08:02:02.243881Z",
     "iopub.status.busy": "2021-12-02T08:02:02.243545Z",
     "iopub.status.idle": "2021-12-02T08:02:02.261271Z",
     "shell.execute_reply": "2021-12-02T08:02:02.260457Z",
     "shell.execute_reply.started": "2021-12-02T08:02:02.243848Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMCustom(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(LSTMCustom, self).__init__()\n",
    "        \n",
    "        # TODO: Implement function\n",
    "        \n",
    "        # set class variables\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define model layers\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # drpout layer\n",
    "        self.dropout = nn.Dropout(dropout)        \n",
    "\n",
    "        # linear layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # TODO: Implement function  \n",
    "        batch_size = nn_input.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(nn_input)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        output = self.dropout(lstm_out)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1, self.output_size)\n",
    "        out = output[:, -1, :] # get last batch of labels       \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return nn.functional.softmax(out), hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "            \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    " \n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T07:06:56.653562Z",
     "iopub.status.busy": "2021-12-02T07:06:56.653177Z",
     "iopub.status.idle": "2021-12-02T07:06:56.666304Z",
     "shell.execute_reply": "2021-12-02T07:06:56.665696Z",
     "shell.execute_reply.started": "2021-12-02T07:06:56.653499Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(filename, decoder):\n",
    "    torch.save(decoder, filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T07:06:56.667844Z",
     "iopub.status.busy": "2021-12-02T07:06:56.667445Z",
     "iopub.status.idle": "2021-12-02T07:06:56.677535Z",
     "shell.execute_reply": "2021-12-02T07:06:56.676714Z",
     "shell.execute_reply.started": "2021-12-02T07:06:56.667814Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches, dataset):\n",
    "    dataloader = dataset.dataloader\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(dataloader, 1):\n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(dataloader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            \n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs.long(), labels.long().squeeze(), hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4} Batch:{}/{}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, batch_i,n_batches,np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "        filename = \"lstm_char_v2_{epoch}_{loss}\".format(epoch=epoch_i, loss=loss)\n",
    "        save_model(filename, rnn)\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T07:06:56.679074Z",
     "iopub.status.busy": "2021-12-02T07:06:56.678703Z",
     "iopub.status.idle": "2021-12-02T07:06:56.692901Z",
     "shell.execute_reply": "2021-12-02T07:06:56.692330Z",
     "shell.execute_reply.started": "2021-12-02T07:06:56.679045Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    rnn.zero_grad()\n",
    "    output, hidden = rnn(inp, hidden)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T08:11:19.317957Z",
     "iopub.status.busy": "2021-12-02T08:11:19.317449Z",
     "iopub.status.idle": "2021-12-02T08:11:19.333085Z",
     "shell.execute_reply": "2021-12-02T08:11:19.332336Z",
     "shell.execute_reply.started": "2021-12-02T08:11:19.317910Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "def predict(dataset, model, next_words):\n",
    "    model.eval()\n",
    "    text = np.random.choice(dataset.characters)[:150] # select random tweet\n",
    "\n",
    "#     words = text.split(' ')\n",
    "    generated =\"\"\n",
    "    print('----- Generating with seed: \"' + text + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.char_to_index[w] for w in text[i:]]])\n",
    "        \n",
    "        hidden = rnn.init_hidden(1)\n",
    "\n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(x, hidden)\n",
    "        last_word_logits = output[-1]\n",
    "\n",
    "        \n",
    "        word_index = np.random.choice(len(last_word_logits), p=last_word_logits.detach().numpy())\n",
    "        generated += \"\" + dataset.index_to_char[word_index]\n",
    "        text = text + dataset.index_to_char[word_index]\n",
    "        \n",
    "        sys.stdout.write(dataset.index_to_char[word_index])\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "    \n",
    "    return generated\n",
    "\n",
    "def train_pred(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, batch_size, dataset, num_epochs, show_every_n_batches, train):\n",
    "    if train == True:\n",
    "\n",
    "        rnn = LSTMCustom(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.3)\n",
    "        if train_on_gpu:\n",
    "            rnn.cuda()\n",
    "\n",
    "        # defining loss and optimization functions for training\n",
    "        optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # training the model\n",
    "        trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches, dataset)\n",
    "\n",
    "        # saving the trained model\n",
    "        save_model('trained_rnn_epoch_20.pt', trained_rnn)\n",
    "        print('Model Trained and Saved')\n",
    "    else:\n",
    "        rnn = torch.load(\"../input/lstmcharv2/lstm_char_v2_9_1.3241478204727173\",map_location=torch.device('cpu'))\n",
    "        predict(dataset, rnn, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-02T08:11:27.375471Z",
     "iopub.status.busy": "2021-12-02T08:11:27.374657Z",
     "iopub.status.idle": "2021-12-02T08:11:46.919205Z",
     "shell.execute_reply": "2021-12-02T08:11:46.918096Z",
     "shell.execute_reply.started": "2021-12-02T08:11:27.375418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 20\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "batch_size=256\n",
    "# Model parameters\n",
    "L = len(dataset.unique_characters)\n",
    "# Vocab size\n",
    "vocab_size = L\n",
    "# Output size\n",
    "output_size = L\n",
    "# Embedding Dimension\n",
    "embedding_dim = 200\n",
    "# Hidden Dimension\n",
    "hidden_dim = 256\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 1000\n",
    "\n",
    "train_pred(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, batch_size, dataset, num_epochs, show_every_n_batches, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFGPT2LMHeadModel, GPT2Model, GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer, GPT2Config\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "# model = GPT2Model.from_pretrained('gpt2', config=config)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\nimport sys\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndf = pd.read_csv(\"/kaggle/input/animedata/outputfile.csv\")\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:34:48.473433Z","iopub.execute_input":"2021-12-05T11:34:48.474195Z","iopub.status.idle":"2021-12-05T11:34:49.792767Z","shell.execute_reply.started":"2021-12-05T11:34:48.474061Z","shell.execute_reply":"2021-12-05T11:34:49.791876Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:34:49.794588Z","iopub.execute_input":"2021-12-05T11:34:49.794819Z","iopub.status.idle":"2021-12-05T11:34:49.800058Z","shell.execute_reply.started":"2021-12-05T11:34:49.794792Z","shell.execute_reply":"2021-12-05T11:34:49.799153Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Dataset():\n    \n    def __init__(self,df):\n        self.df = df\n        self.characters, self.unique_characters = self.load_characters()\n        self.index_to_char = dict((int(i), c) for i, c in enumerate(self.unique_characters))\n        self.char_to_index = dict((c, int(i)) for i, c in enumerate(self.unique_characters))\n        \n        self.maxLen = 150\n        \n        self.x ,self.y = self.x_y_data()\n        \n        self.dataloader = self.batch_data(self.x, self.y, self.maxLen, 256)\n        \n    \n    def load_characters(self):\n        # lowercase all\n        df = self.df.drop_duplicates(subset=['synopsis'])\n        df['synopsis'] = df.synopsis.replace('\\n','', regex=True)\n        df['synopsis'] = df.synopsis.replace('\\r','', regex=True)\n        df['synopsis'] = df.synopsis.replace('[^\\w\\s]','', regex=True)\n\n        text = df['synopsis'][df['synopsis'].map(len)>150]\n        \n        text = text.sample(frac = 0.5)\n\n        chars = sorted(list(set(''.join(text))))\n\n        for c in range(len(chars)):\n            if chars[c] == \"z\":\n                for i in chars[c+1:]:\n                    text = text.str.replace(i,'')\n        \n        chars = sorted(list(set(''.join(text))))\n        \n        print(len(chars))\n        return text, chars\n\n    def id_sentence(self, sentence):\n        return [self.char_to_index[char] for char in sentence]\n            \n    \n    def x_y_data(self):\n        # cut the text in semi-redundant sequences of maxlen characters\n        maxlen = 150\n        step = 1\n        sentences = []\n        next_chars = []\n        for x in self.characters:\n            for i in range(0, len(x) - maxlen, step):\n                sentences.append(self.id_sentence(x[i: i + maxlen]))\n                next_chars.append(self.id_sentence(x[i + maxlen]))\n        print('nb sequences:', len(sentences))\n        \n        return sentences, next_chars\n    \n    def batch_data(self, x,y, sequence_length, batch_size):\n        \"\"\"\n        Batch the neural network data using DataLoader\n        :param words: The word ids of the scripts\n        :param sequence_length: The sequence length of each batch\n        :param batch_size: The size of each batch; the number of sequences in a batch\n        :return: DataLoader with batched data\n        \"\"\"\n        # TODO: Implement function\n\n        data = TensorDataset(torch.Tensor(np.asarray(x)), torch.Tensor(np.asarray(y)))\n        \n        data_loader = torch.utils.data.DataLoader(data, shuffle=False, batch_size=batch_size)\n        \n        print('data on cuda?', data_loader.is_cuda)\n\n        # return a dataloader\n        return data_loader\n    \n    def __len__(self):\n        return self.x.shape[0]\n    \n    def __getitem__(self,idx):\n        return self.x[idx],self.y[idx]","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:34:49.801911Z","iopub.execute_input":"2021-12-05T11:34:49.802245Z","iopub.status.idle":"2021-12-05T11:34:49.825783Z","shell.execute_reply.started":"2021-12-05T11:34:49.802186Z","shell.execute_reply":"2021-12-05T11:34:49.825192Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"[1,2,3,4,5,6]\n\n[\n    [1,2,3],\n    [2,3,4],\n    [3,4,5],\n    [4,5,6]\n]\n\n[\n    [4],\n    [5],\n    [6],\n    [END]\n]","metadata":{"execution":{"iopub.status.busy":"2021-12-02T07:05:04.942935Z","iopub.execute_input":"2021-12-02T07:05:04.943242Z","iopub.status.idle":"2021-12-02T07:05:04.955561Z","shell.execute_reply.started":"2021-12-02T07:05:04.943217Z","shell.execute_reply":"2021-12-02T07:05:04.954982Z"}}},{"cell_type":"code","source":"import pickle\n# dataset = Dataset(df)\n\n# #ensure data persistence, because we arent using the full dataset\n# with open('dataset_65.pkl', 'wb') as outp:\n#     pickle.dump(dataset, outp, pickle.HIGHEST_PROTOCOL)\n\nwith open('../input/chardatasetclass65/dataset_65.pkl', 'rb') as inp:\n    dataset = pickle.load(inp)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:34:49.827022Z","iopub.execute_input":"2021-12-05T11:34:49.827553Z","iopub.status.idle":"2021-12-05T11:35:22.825719Z","shell.execute_reply.started":"2021-12-05T11:34:49.827522Z","shell.execute_reply":"2021-12-05T11:35:22.824936Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dataset.dataloader","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:22.827991Z","iopub.execute_input":"2021-12-05T11:35:22.828231Z","iopub.status.idle":"2021-12-05T11:35:22.833895Z","shell.execute_reply.started":"2021-12-05T11:35:22.828184Z","shell.execute_reply":"2021-12-05T11:35:22.833104Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass LSTMCustom(nn.Module):\n    \n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n        \"\"\"\n        Initialize the PyTorch RNN Module\n        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n        :param output_size: The number of output dimensions of the neural network\n        :param embedding_dim: The size of embeddings, should you choose to use them        \n        :param hidden_dim: The size of the hidden layer outputs\n        :param dropout: dropout to add in between LSTM/GRU layers\n        \"\"\"\n        super(LSTMCustom, self).__init__()\n        \n        # TODO: Implement function\n        \n        # set class variables\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # define model layers\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n        \n        # drpout layer\n        self.dropout = nn.Dropout(dropout)        \n\n        # linear layer\n        self.fc = nn.Linear(hidden_dim, output_size)\n    \n    def forward(self, nn_input, hidden):\n        \"\"\"\n        Forward propagation of the neural network\n        :param nn_input: The input to the neural network\n        :param hidden: The hidden state        \n        :return: Two Tensors, the output of the neural network and the latest hidden state\n        \"\"\"\n        # TODO: Implement function  \n        batch_size = nn_input.size(0)\n\n        # embeddings and lstm_out\n        embeds = self.embedding(nn_input)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        output = self.dropout(lstm_out)\n        output = self.fc(output)\n        \n        # reshape to be batch_size first\n        output = output.view(batch_size, -1, self.output_size)\n        out = output[:, -1, :] # get last batch of labels       \n        # return one batch of output word scores and the hidden state\n        return nn.functional.softmax(out), hidden\n    \n    \n    def init_hidden(self, batch_size):\n        '''\n        Initialize the hidden state of an LSTM/GRU\n        :param batch_size: The batch_size of the hidden state\n        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n        '''\n        # Implement function\n        weight = next(self.parameters()).data.to(device)\n        \n        # initialize hidden state with zero weights, and move to GPU if available\n            \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n \n        \n        return hidden","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:22.835370Z","iopub.execute_input":"2021-12-05T11:35:22.835774Z","iopub.status.idle":"2021-12-05T11:35:22.850607Z","shell.execute_reply.started":"2021-12-05T11:35:22.835744Z","shell.execute_reply":"2021-12-05T11:35:22.849735Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def save_model(filename, decoder):\n    torch.save(decoder, filename) ","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:22.851802Z","iopub.execute_input":"2021-12-05T11:35:22.852495Z","iopub.status.idle":"2021-12-05T11:35:22.866230Z","shell.execute_reply.started":"2021-12-05T11:35:22.852428Z","shell.execute_reply":"2021-12-05T11:35:22.865655Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches, dataset):\n    dataloader = dataset.dataloader\n    batch_losses = []\n    \n    rnn.train()\n\n    print(\"Training for %d epoch(s)...\" % n_epochs)\n    for epoch_i in range(1, n_epochs + 1):\n        \n        # initialize hidden state\n        hidden = rnn.init_hidden(batch_size)\n        \n        for batch_i, (inputs, labels) in enumerate(dataloader, 1):\n            # make sure you iterate over completely full batches, only\n            n_batches = len(dataloader.dataset)//batch_size\n            if(batch_i > n_batches):\n                break\n            \n            # forward, back prop\n            \n            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs.long(), labels.long().squeeze(), hidden)          \n            # record loss\n            batch_losses.append(loss)\n\n            # printing loss stats\n            if batch_i % show_every_n_batches == 0:\n                print('Epoch: {:>4}/{:<4} Batch:{}/{}  Loss: {}\\n'.format(\n                    epoch_i, n_epochs, batch_i,n_batches,np.average(batch_losses)))\n                batch_losses = []\n        filename = \"lstm_char_v2_{epoch}_{loss}\".format(epoch=epoch_i, loss=loss)\n        save_model(filename, rnn)\n\n    # returns a trained rnn\n    return rnn","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:22.867361Z","iopub.execute_input":"2021-12-05T11:35:22.868005Z","iopub.status.idle":"2021-12-05T11:35:22.878661Z","shell.execute_reply.started":"2021-12-05T11:35:22.867974Z","shell.execute_reply":"2021-12-05T11:35:22.878119Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n    \"\"\"\n    Forward and backward propagation on the neural network\n    :param decoder: The PyTorch Module that holds the neural network\n    :param decoder_optimizer: The PyTorch optimizer for the neural network\n    :param criterion: The PyTorch loss function\n    :param inp: A batch of input to the neural network\n    :param target: The target output for the batch of input\n    :return: The loss and the latest hidden state Tensor\n    \"\"\"\n    \n    # TODO: Implement Function\n    \n    # move data to GPU, if available\n    \n    if train_on_gpu:\n        inp, target = inp.cuda(), target.cuda()\n    \n    # perform backpropagation and optimization\n    hidden = tuple([each.data for each in hidden])\n    rnn.zero_grad()\n    output, hidden = rnn(inp, hidden)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n\n    # return the loss over a batch and the hidden state produced by our model\n    return loss.item(), hidden","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:22.879735Z","iopub.execute_input":"2021-12-05T11:35:22.880034Z","iopub.status.idle":"2021-12-05T11:35:22.895710Z","shell.execute_reply.started":"2021-12-05T11:35:22.880008Z","shell.execute_reply":"2021-12-05T11:35:22.894918Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Check for a GPU\ntrain_on_gpu = torch.cuda.is_available()\nprint(train_on_gpu)\n\ndef predict(dataset, model, next_words):\n    model.eval()\n    og_text = np.random.choice(dataset.characters)[:next_words]\n    text = og_text[:150] # select random tweet\n\n#     words = text.split(' ')\n    generated =\"\"\n    print('----- Generating with seed: \"' + text + '\"')\n    sys.stdout.write(generated)\n    for i in range(0, next_words):\n        x = torch.tensor([[dataset.char_to_index[w] for w in text[i:]]]).to(device)\n        \n        hidden = model.init_hidden(1)\n\n        # get the output of the rnn\n        output, _ = model(x, hidden)\n        last_word_logits = output[-1]\n\n        \n        word_index = np.random.choice(len(last_word_logits), p=last_word_logits.detach().numpy())\n        generated += \"\" + dataset.index_to_char[word_index]\n        text = text + dataset.index_to_char[word_index]\n        \n        sys.stdout.write(dataset.index_to_char[word_index])\n        sys.stdout.flush()\n    print()\n    return og_text, generated\n\ndef train_pred(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, batch_size, dataset, num_epochs, show_every_n_batches, train):\n    rnn = LSTMCustom(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.3)\n    \n    if train == True:\n        if train_on_gpu:\n            rnn.cuda()\n\n        # defining loss and optimization functions for training\n        optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n        criterion = nn.CrossEntropyLoss()\n\n        # training the model\n        trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches, dataset)\n\n        # saving the trained model\n        save_model('trained_rnn_epoch_20.pt', trained_rnn)\n        print('Model Trained and Saved')\n    else:\n        rnn = torch.load(\"../input/lstmcharv2/lstm_char_v2_9_1.3241478204727173\",map_location=torch.device('cpu'))\n        original, generated = predict(dataset, rnn, 500)\n        return original, generated","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-05T11:35:22.897108Z","iopub.execute_input":"2021-12-05T11:35:22.897865Z","iopub.status.idle":"2021-12-05T11:35:22.914821Z","shell.execute_reply.started":"2021-12-05T11:35:22.897821Z","shell.execute_reply":"2021-12-05T11:35:22.913919Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Training parameters\n# Number of Epochs\nnum_epochs = 20\n# Learning Rate\nlearning_rate = 0.001\nbatch_size=256\n# Model parameters\nL = len(dataset.unique_characters)\n# Vocab size\nvocab_size = L\n# Output size\noutput_size = L\n# Embedding Dimension\nembedding_dim = 200\n# Hidden Dimension\nhidden_dim = 256\n# Number of RNN Layers\nn_layers = 2\n# Show stats for every n number of batches\nshow_every_n_batches = 1000\n\n\noutput = train_pred(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, batch_size, dataset, num_epochs, show_every_n_batches, False)\nprint(len(output[0]), len(output[1]))","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:22.916047Z","iopub.execute_input":"2021-12-05T11:35:22.916274Z","iopub.status.idle":"2021-12-05T11:35:39.597595Z","shell.execute_reply.started":"2021-12-05T11:35:22.916243Z","shell.execute_reply":"2021-12-05T11:35:39.595703Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation (Perplexity)","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Model, GPT2LMHeadModel\nfrom transformers import GPT2Tokenizer, GPT2Config\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nconfig = GPT2Config.from_pretrained('gpt2')\n# model = GPT2Model.from_pretrained('gpt2', config=config)\nmodel = GPT2LMHeadModel.from_pretrained('gpt2', config=config)\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:39.599138Z","iopub.execute_input":"2021-12-05T11:35:39.599655Z","iopub.status.idle":"2021-12-05T11:35:46.860174Z","shell.execute_reply.started":"2021-12-05T11:35:39.599610Z","shell.execute_reply":"2021-12-05T11:35:46.859310Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import math\n\ndef calculatePerplexity(sentence):\n        input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0) \n        with torch.no_grad():\n            outputs = model(input_ids, labels=input_ids)\n#         print(outputs)\n        loss, logits = outputs[:2]\n#         print(loss)\n        return math.exp(loss)","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:46.862074Z","iopub.execute_input":"2021-12-05T11:35:46.862703Z","iopub.status.idle":"2021-12-05T11:35:46.870034Z","shell.execute_reply.started":"2021-12-05T11:35:46.862656Z","shell.execute_reply":"2021-12-05T11:35:46.869262Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"ppl = calculatePerplexity(output[1])\nppl","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:46.873242Z","iopub.execute_input":"2021-12-05T11:35:46.874223Z","iopub.status.idle":"2021-12-05T11:35:47.251492Z","shell.execute_reply.started":"2021-12-05T11:35:46.874159Z","shell.execute_reply":"2021-12-05T11:35:47.250641Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import SmoothingFunction, corpus_bleu, sentence_bleu\n\n\ndef bleu(ref, gen):\n    ''' \n    calculate pair wise bleu score. uses nltk implementation\n    Args:\n        references : a list of reference sentences \n        candidates : a list of candidate(generated) sentences\n    Returns:\n        bleu score(float)\n    '''\n    ref_bleu = []\n    gen_bleu = []\n    gen_bleu = gen.split(\" \")\n    ref_bleu.append(ref.split(\" \"))\n    cc = SmoothingFunction()\n    score_bleu = sentence_bleu(ref_bleu, gen_bleu, weights=(0, 1, 0, 0), smoothing_function=cc.method4)\n    return score_bleu","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:47.252789Z","iopub.execute_input":"2021-12-05T11:35:47.253667Z","iopub.status.idle":"2021-12-05T11:35:47.471662Z","shell.execute_reply.started":"2021-12-05T11:35:47.253623Z","shell.execute_reply":"2021-12-05T11:35:47.470906Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#rouge scores for a reference/generated sentence pair\n#source google seq2seq source code.\n\nimport itertools\n\n#supporting function\ndef _split_into_words(sentences):\n    \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n    return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n\n#supporting function\ndef _get_word_ngrams(n, sentences):\n    \"\"\"Calculates word n-grams for multiple sentences.\n    \"\"\"\n    assert len(sentences) > 0\n    assert n > 0\n\n    words = _split_into_words(sentences)\n    return _get_ngrams(n, words)\n\n#supporting function\ndef _get_ngrams(n, text):\n    \"\"\"Calcualtes n-grams.\n    Args:\n        n: which n-grams to calculate\n        text: An array of tokens\n    Returns:\n        A set of n-grams\n    \"\"\"\n    ngram_set = set()\n    text_length = len(text)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram_set.add(tuple(text[i:i + n]))\n    return ngram_set\n\ndef rouge_n(reference_sentences, evaluated_sentences, n=2):\n    \"\"\"\n    Computes ROUGE-N of two text collections of sentences.\n    Source: http://research.microsoft.com/en-us/um/people/cyl/download/\n    papers/rouge-working-note-v1.3.1.pdf\n    Args:\n        evaluated_sentences: The sentences that have been picked by the summarizer\n        reference_sentences: The sentences from the referene set\n        n: Size of ngram.  Defaults to 2.\n    Returns:\n        recall rouge score(float)\n    Raises:\n        ValueError: raises exception if a param has len <= 0\n    \"\"\"\n    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n        raise ValueError(\"Collections must contain at least 1 sentence.\")\n\n    evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n    reference_ngrams = _get_word_ngrams(n, reference_sentences)\n    reference_count = len(reference_ngrams)\n    evaluated_count = len(evaluated_ngrams)\n\n    # Gets the overlapping ngrams between evaluated and reference\n    overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n    overlapping_count = len(overlapping_ngrams)\n\n    # Handle edge case. This isn't mathematically correct, but it's good enough\n    if evaluated_count == 0:\n        precision = 0.0\n    else:\n        precision = overlapping_count / evaluated_count\n\n    if reference_count == 0:\n        recall = 0.0\n    else:\n        recall = overlapping_count / reference_count\n\n    f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n\n    #just returning recall count in rouge, useful for our purpose\n    return recall","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:47.473048Z","iopub.execute_input":"2021-12-05T11:35:47.473279Z","iopub.status.idle":"2021-12-05T11:35:47.485795Z","shell.execute_reply.started":"2021-12-05T11:35:47.473252Z","shell.execute_reply":"2021-12-05T11:35:47.485048Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def bleurouge(ref, gen, n_for_rouge = 2):\n    '''\n    Args:\n        ref_file_path (string) : reference file path -> file containing the reference sentences on each line\n        gen_file_path (string) : model generated file path -> containing corresponding generated sentences(to reference sentences) on each line\n    \n    Returns:\n        A list containing [bleu, rouge, meteor, ter]\n    '''\n    bleu_score = bleu(ref, gen)\n    rouge_score = rouge_n(ref, gen, n=n_for_rouge)\n    return [bleu_score, rouge_score]","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:47.486687Z","iopub.execute_input":"2021-12-05T11:35:47.487670Z","iopub.status.idle":"2021-12-05T11:35:47.503286Z","shell.execute_reply.started":"2021-12-05T11:35:47.487620Z","shell.execute_reply":"2021-12-05T11:35:47.502448Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"scores = bleurouge(output[0], output[1])\nscores","metadata":{"execution":{"iopub.status.busy":"2021-12-05T11:35:47.504739Z","iopub.execute_input":"2021-12-05T11:35:47.505260Z","iopub.status.idle":"2021-12-05T11:35:47.520124Z","shell.execute_reply.started":"2021-12-05T11:35:47.505192Z","shell.execute_reply":"2021-12-05T11:35:47.519334Z"},"trusted":true},"execution_count":18,"outputs":[]}]}